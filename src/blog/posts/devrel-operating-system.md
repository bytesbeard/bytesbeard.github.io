---
title: DevRel as an Operating System
description: Flywheels, Budget Confidence, and Compounding Trust.
date: "2025-06-15"
tags: [community, devrel]
---


There's a phase every DevRel program goes through.

Phase 1: **proving it exists**
Phase 2: **proving it matters**
Phase 3: **proving it compounds**

Most teams never reach Phase 3 because they're stuck defending the work instead of operationalizing it.

So I started describing DevRel differently:

> **DevRel is not marketing. DevRel is organizational infrastructure.**

And infrastructure is evaluated on reliability, usefulness, and compounding — not vibes.

### The flywheel (what we're actually building)

A builder community flywheel looks like this:

1. **Trust** → people believe showing up is worth it
2. **Safety** → people share unfinished work
3. **Sharing** → demos, learnings, mistakes become visible
4. **Feedback** → builders improve faster
5. **Collaboration** → people build together
6. **Outcomes** → projects ship, hires happen, champions emerge
7. **Proof** → trust increases

Most teams only measure step (3) because it's easy ("we had demos").
But the health is in steps (1), (2), (4), (5).

### Why "budget confidence" matters more than ROI

I've seen DevRel get cut even when it clearly mattered — because leadership couldn't predict it.

ROI is often a trap because DevRel returns are non-linear and delayed.

What leaders actually want is:

* "If we invest X this quarter, what's the range of outcomes we can expect?"
* "What are we buying: awareness, pipeline, trust, hires, adoption?"
* "What do we stop doing if it doesn't work?"

So I started framing DevRel like this:

> **Don't sell ROI. Sell budget confidence.**
> "Here's what we will produce, reliably, with this investment—and here's how we'll know if it's working."

### The DevRel Operating System (DOS)

My DOS has 4 parts:

#### 1) The Program Design

* what's the format and why
* what behavior does it reward
* what behavior does it reject

AI Tinkerers rewards showing work.
It rejects performance for clout.

That alone shapes the metrics.

#### 2) The Mechanisms

* demo slot contract
* speaker pipeline board
* 48-hour retro
* follow-up minimum
* weekly pulse review

Mechanisms are the "engine room".

#### 3) The Metrics (small, brutal, decision-driving)

* returning builder rate
* first-time → demo conversion
* repeat speaker rate
* collaboration sparks
* sponsor renewal
* time-to-ship

This is the dashboard. Not the product.

#### 4) The Narrative Layer

Here's the part most teams miss.

Leadership doesn't just need numbers.
They need **a story that explains the numbers**.

Not marketing storytelling. Operational storytelling.

Example:

* "Returning builder rate increased because we made WIP demos explicit and reduced stage pressure."
* "Repeat speaker rate dropped because we over-indexed on big names and ignored quiet builders."
* "Collaboration sparks increased because we added structured networking prompts."

Narratives make metrics legible.

### The "recent event" perspective shift

Here's what I keep relearning after events:

The outcome isn't "people attended".
The outcome is "builders trusted the system".

And trust compounds when:

* the format stays consistent
* feedback stays honest
* and follow-up stays real

So the real question becomes:

> **What do we do every month that makes trust inevitable?**

That's an operating system question.

### Closing

If your DevRel program is measured like content marketing, it will be forced to behave like content marketing.

If your DevRel program is measured like infrastructure—reliability, compounding, capability creation — it starts behaving like infrastructure.

That's the line I've crossed to move into the realm of showing how Devrel sticks in an organization.

And once you cross it, you don't obsess over impressions anymore.

You obsess over:

* returning builders
* new demos
* repeat speakers
* collaboration
* and trust that compounds

That's how tech ecosystems are built.