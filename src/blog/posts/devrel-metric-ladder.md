---
title: The DevRel Metric Ladder
description: From Activity to Compounding.
date: "2025-03-26"
tags: [community, devrel]
---


There's a quiet tragedy in DevRel reporting.

Some teams measure everything.
Some measure nothing.
Most measure what's easiest and then wonder why the work feels "unproven".

The fix isn't "more metrics". The fix is **a ladder**.

### The Metric Ladder

Here's the ladder I've used (and refined) across AWS-era community work and now AI Tinkerers:

#### Level 1 — Activity (the "I did stuff" layer)

* posts published
* events hosted
* talks delivered
* workshops run
* booth scans, swag given

This layer is useful for capacity planning and operational reporting. One does not use this to show as proof of impact.

#### Level 2 — Participation (the "people showed up" layer)

* RSVPs
* attendance
* watch time
* questions asked
* number of demos

Better. But still not reaching the impact rung of the ladder.

#### Level 3 — Engagement Quality (the "builders actually did something" layer)

* returning attendees
* repeat speakers
* hallway conversations that become follow-ups
* number of people who share work-in-progress
* number of "asks" raised (help needed, collaboration requests)

This is where communities become ecosystems.

#### Level 4 — Outcomes (the "something changed" layer)

* project collaborations formed
* hires influenced (not "credited," influenced)
* customer conversations initiated
* POCs started because of a talk
* internal champions created inside companies

The intial outcomes are messy. That's normal.

#### Level 5 — Compounding (the "the system now runs better" layer)

* more demos without you begging
* more volunteers without you chasing
* sponsors renewing because value is obvious
* stronger speaker pipeline
* audience self-organizing into sub-groups
* builders mentoring builders

Compounding is the goal! Because compounding is what turns DevRel from an expense/cost centre into an engine.

### Why teams get stuck

Most teams get stuck at Level 2 because Levels 3–5 require:

* intentional design
* follow-up
* capture
* and frankly… humility

You can't measure outcomes if you never asked people what happened next. You can't measure compounding if your program isn't built to repeat.

And no, number of attendees on your meetup.com dashboard don't tell the real story of impact through your communities.

### The "AI Tinkerers test" for whether your metric ladder is real

After a demo night, I ask:

1. **Who is likely to demo next time because they saw this?**
2. **Who did we connect today that will still matter in 30 days?**
3. **What do we repeat next month because it worked?**
4. **What do we kill because it wasted builder attention?**

If your metrics can't answer those, you don't have metrics. You have trivia.

### The practical set (keep it small)

If you want a minimal set that covers the ladder without turning your life into spreadsheets, use:

* **Returning builders %** (Level 3)
* **Repeat speakers %** (Level 3)
* **# of demos per event** (Level 2→3 bridge)
* **# of "introductions that converted" within 30 days** (Level 4)
* **Sponsor renewal rate / sponsor NPS** (Level 5 signal)
* **Time-to-program** (how fast you can go from idea → event shipped) (operational compounding)

Yes, you'll still track RSVP/attendance. But you stop worshipping them.

### The real shift

The shift is moving from:

> "How do I prove DevRel worked?"

to:

> "How do I design DevRel so it predictably produces outcomes?"

That design problem is mechanisms-forcing functions—operating rhythms.

That's what the next article is about.
